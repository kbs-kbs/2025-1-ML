앙상블
- 여러 모델을 결합해 더 일반화된(과적합에 덜 취약한) 모델을 만드는 것
- 기법: 1. 보팅(voting), 2. 배깅(bagging), 3. 부스팅(boosting), 4. 스태킹(stacking)
- 각 기법이 일반화를 달성하는 구체적 방식이 다름

1. 보팅(Voting): 여러 개의 서로 다른 모델(이질적 모델)을 같은 데이터에 학습시켜, 예측 결과를 다수결(분류)이나 평균(회귀)으로 집계
   - 보팅은 분류 분제와 회귀 문제에서 의미가 다름
   - 분류 문제: 다수결 투표(하드 보팅)
   - 회귀 문제: 다수결 투표가 아닌 각 모델의 예측값을 산술적으로 평균내는 방식을 의미 (소프트 보팅)
3. 배깅(Bagging): 같은 알고리즘의 모델 여러 개를, 각기 다른 부트스트랩 샘플(중복 허용 랜덤 샘플)로 학습시켜 예측 결과를 결합합니다.
   - 예측 결과를 다수결(분류)이나 평균(회귀)으로 집계한다는 점은 보팅과 배깅의 공통점
   - OOB(out-of-bag) 샘플: 앙상블 전체가 아니라 개별 모델 관점에서 사용되지 않은 샘플
  
>[!note]
> 1번 샘플: 2, 3번 모델이 OOB(즉, 1번 샘플을 학습에 사용하지 않음)
> → 2, 3번 모델이 1번 샘플에 대해 예측
> → 두 모델의 예측을 집계(분류라면 다수결, 회귀라면 평균)
> → 이 결과가 1번 샘플의 OOB 예측값
>
> 2번 샘플: 3, 1번 모델이 OOB
> → 3, 1번 모델이 2번 샘플에 대해 예측
> → 두 모델의 예측을 집계
> → 이 결과가 2번 샘플의 OOB 예측값
>
> 3번 샘플: 2번 모델만 OOB
> → 2번 모델이 3번 샘플에 대해 예측
> → 2번 모델의 예측이 3번 샘플의 OOB 예측값
>
> 최종 OOB 평가
> 모든 샘플에 대해 위와 같이 OOB 예측값을 구한 뒤,
>
> OOB 예측값과 실제 정답을 비교하여 전체 OOB 정확도(분류)나 평균 제곱오차(회귀) 등으로 앙상블 전체의 OOB 성능을 평가합니다
5. 부스팅(Boosting): 약한 모델을 순차적으로 연결하며, 이전 모델의 오차에 더 집중해 다음 모델을 학습시킵니다.
6. 스태킹(Stacking): 여러 모델의 예측 결과를 다시 입력값으로 삼아, 또 다른 메타 모델이 최종 예측을 만듭니다.

